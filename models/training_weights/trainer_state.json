{
  "best_global_step": 2000,
  "best_metric": 0.8709157109260559,
  "best_model_checkpoint": "/kaggle/working/result/checkpoint-2000",
  "epoch": 0.7118071002758253,
  "eval_steps": 2000,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003559035501379126,
      "grad_norm": 9.057130813598633,
      "learning_rate": 0.0002998718405126379,
      "loss": 41.9143,
      "step": 10
    },
    {
      "epoch": 0.007118071002758252,
      "grad_norm": 7.813785552978516,
      "learning_rate": 0.0002996582413670345,
      "loss": 13.4391,
      "step": 20
    },
    {
      "epoch": 0.010677106504137378,
      "grad_norm": 6.590574741363525,
      "learning_rate": 0.0002994446422214311,
      "loss": 11.6059,
      "step": 30
    },
    {
      "epoch": 0.014236142005516504,
      "grad_norm": 10.322345733642578,
      "learning_rate": 0.0002992310430758277,
      "loss": 10.3111,
      "step": 40
    },
    {
      "epoch": 0.017795177506895632,
      "grad_norm": 8.797857284545898,
      "learning_rate": 0.00029901744393022423,
      "loss": 9.8066,
      "step": 50
    },
    {
      "epoch": 0.021354213008274756,
      "grad_norm": 10.338685035705566,
      "learning_rate": 0.00029880384478462083,
      "loss": 9.2459,
      "step": 60
    },
    {
      "epoch": 0.024913248509653884,
      "grad_norm": 12.101725578308105,
      "learning_rate": 0.0002985902456390174,
      "loss": 8.8613,
      "step": 70
    },
    {
      "epoch": 0.02847228401103301,
      "grad_norm": 10.404959678649902,
      "learning_rate": 0.00029837664649341397,
      "loss": 8.8183,
      "step": 80
    },
    {
      "epoch": 0.032031319512412136,
      "grad_norm": 8.641939163208008,
      "learning_rate": 0.00029816304734781057,
      "loss": 8.818,
      "step": 90
    },
    {
      "epoch": 0.035590355013791264,
      "grad_norm": 8.743823051452637,
      "learning_rate": 0.00029794944820220716,
      "loss": 8.7119,
      "step": 100
    },
    {
      "epoch": 0.03914939051517039,
      "grad_norm": 9.190400123596191,
      "learning_rate": 0.00029773584905660376,
      "loss": 8.5719,
      "step": 110
    },
    {
      "epoch": 0.04270842601654951,
      "grad_norm": 8.398188591003418,
      "learning_rate": 0.0002975222499110003,
      "loss": 8.4795,
      "step": 120
    },
    {
      "epoch": 0.04626746151792864,
      "grad_norm": 8.962902069091797,
      "learning_rate": 0.00029730865076539695,
      "loss": 8.6522,
      "step": 130
    },
    {
      "epoch": 0.04982649701930777,
      "grad_norm": 10.322561264038086,
      "learning_rate": 0.0002970950516197935,
      "loss": 8.3084,
      "step": 140
    },
    {
      "epoch": 0.053385532520686896,
      "grad_norm": 8.249839782714844,
      "learning_rate": 0.0002968814524741901,
      "loss": 8.1809,
      "step": 150
    },
    {
      "epoch": 0.05694456802206602,
      "grad_norm": 10.654955863952637,
      "learning_rate": 0.00029666785332858664,
      "loss": 8.1979,
      "step": 160
    },
    {
      "epoch": 0.060503603523445144,
      "grad_norm": 10.81652545928955,
      "learning_rate": 0.00029645425418298323,
      "loss": 8.2268,
      "step": 170
    },
    {
      "epoch": 0.06406263902482427,
      "grad_norm": 11.010881423950195,
      "learning_rate": 0.00029624065503737983,
      "loss": 8.164,
      "step": 180
    },
    {
      "epoch": 0.06762167452620339,
      "grad_norm": 10.720491409301758,
      "learning_rate": 0.00029602705589177637,
      "loss": 8.11,
      "step": 190
    },
    {
      "epoch": 0.07118071002758253,
      "grad_norm": 9.0181303024292,
      "learning_rate": 0.000295813456746173,
      "loss": 8.0304,
      "step": 200
    },
    {
      "epoch": 0.07473974552896165,
      "grad_norm": 11.02503776550293,
      "learning_rate": 0.00029559985760056957,
      "loss": 8.1485,
      "step": 210
    },
    {
      "epoch": 0.07829878103034078,
      "grad_norm": 9.707891464233398,
      "learning_rate": 0.00029538625845496616,
      "loss": 7.8231,
      "step": 220
    },
    {
      "epoch": 0.0818578165317199,
      "grad_norm": 7.4261627197265625,
      "learning_rate": 0.0002951726593093627,
      "loss": 7.9549,
      "step": 230
    },
    {
      "epoch": 0.08541685203309902,
      "grad_norm": 8.763772010803223,
      "learning_rate": 0.00029495906016375936,
      "loss": 7.7525,
      "step": 240
    },
    {
      "epoch": 0.08897588753447816,
      "grad_norm": 7.859776973724365,
      "learning_rate": 0.0002947454610181559,
      "loss": 7.6879,
      "step": 250
    },
    {
      "epoch": 0.09253492303585728,
      "grad_norm": 7.6574249267578125,
      "learning_rate": 0.0002945318618725525,
      "loss": 8.027,
      "step": 260
    },
    {
      "epoch": 0.09609395853723642,
      "grad_norm": 10.191217422485352,
      "learning_rate": 0.00029431826272694904,
      "loss": 7.828,
      "step": 270
    },
    {
      "epoch": 0.09965299403861554,
      "grad_norm": 11.97705364227295,
      "learning_rate": 0.00029410466358134564,
      "loss": 8.0435,
      "step": 280
    },
    {
      "epoch": 0.10321202953999466,
      "grad_norm": 8.374275207519531,
      "learning_rate": 0.00029389106443574223,
      "loss": 7.6722,
      "step": 290
    },
    {
      "epoch": 0.10677106504137379,
      "grad_norm": 9.424551010131836,
      "learning_rate": 0.00029367746529013883,
      "loss": 7.8201,
      "step": 300
    },
    {
      "epoch": 0.11033010054275291,
      "grad_norm": 10.727130889892578,
      "learning_rate": 0.0002934638661445354,
      "loss": 7.8017,
      "step": 310
    },
    {
      "epoch": 0.11388913604413203,
      "grad_norm": 7.522421360015869,
      "learning_rate": 0.00029325026699893197,
      "loss": 7.8228,
      "step": 320
    },
    {
      "epoch": 0.11744817154551117,
      "grad_norm": 9.657666206359863,
      "learning_rate": 0.00029303666785332857,
      "loss": 7.7029,
      "step": 330
    },
    {
      "epoch": 0.12100720704689029,
      "grad_norm": 9.08447265625,
      "learning_rate": 0.0002928230687077251,
      "loss": 7.6627,
      "step": 340
    },
    {
      "epoch": 0.12456624254826942,
      "grad_norm": 8.271766662597656,
      "learning_rate": 0.00029260946956212176,
      "loss": 7.7535,
      "step": 350
    },
    {
      "epoch": 0.12812527804964854,
      "grad_norm": 7.6269307136535645,
      "learning_rate": 0.0002923958704165183,
      "loss": 7.6231,
      "step": 360
    },
    {
      "epoch": 0.13168431355102767,
      "grad_norm": 7.653158187866211,
      "learning_rate": 0.0002921822712709149,
      "loss": 7.7127,
      "step": 370
    },
    {
      "epoch": 0.13524334905240679,
      "grad_norm": 8.679158210754395,
      "learning_rate": 0.0002919686721253115,
      "loss": 7.6364,
      "step": 380
    },
    {
      "epoch": 0.13880238455378593,
      "grad_norm": 9.122912406921387,
      "learning_rate": 0.0002917764328942684,
      "loss": 7.7489,
      "step": 390
    },
    {
      "epoch": 0.14236142005516506,
      "grad_norm": 8.22773551940918,
      "learning_rate": 0.000291562833748665,
      "loss": 7.404,
      "step": 400
    },
    {
      "epoch": 0.14592045555654418,
      "grad_norm": 7.154226303100586,
      "learning_rate": 0.0002913492346030616,
      "loss": 7.7567,
      "step": 410
    },
    {
      "epoch": 0.1494794910579233,
      "grad_norm": 7.093035697937012,
      "learning_rate": 0.0002911356354574581,
      "loss": 7.4032,
      "step": 420
    },
    {
      "epoch": 0.15303852655930242,
      "grad_norm": 6.582706451416016,
      "learning_rate": 0.0002909220363118547,
      "loss": 7.7069,
      "step": 430
    },
    {
      "epoch": 0.15659756206068157,
      "grad_norm": 8.320416450500488,
      "learning_rate": 0.0002907084371662513,
      "loss": 7.5789,
      "step": 440
    },
    {
      "epoch": 0.1601565975620607,
      "grad_norm": 7.466945648193359,
      "learning_rate": 0.0002904948380206479,
      "loss": 7.6379,
      "step": 450
    },
    {
      "epoch": 0.1637156330634398,
      "grad_norm": 7.728466510772705,
      "learning_rate": 0.00029028123887504445,
      "loss": 7.5112,
      "step": 460
    },
    {
      "epoch": 0.16727466856481893,
      "grad_norm": 7.154544353485107,
      "learning_rate": 0.00029006763972944105,
      "loss": 7.4463,
      "step": 470
    },
    {
      "epoch": 0.17083370406619805,
      "grad_norm": 7.1556010246276855,
      "learning_rate": 0.00028985404058383765,
      "loss": 7.7575,
      "step": 480
    },
    {
      "epoch": 0.1743927395675772,
      "grad_norm": 7.22203254699707,
      "learning_rate": 0.0002896404414382342,
      "loss": 7.4535,
      "step": 490
    },
    {
      "epoch": 0.17795177506895632,
      "grad_norm": 8.403566360473633,
      "learning_rate": 0.0002894268422926308,
      "loss": 7.4914,
      "step": 500
    },
    {
      "epoch": 0.18151081057033544,
      "grad_norm": 6.242124080657959,
      "learning_rate": 0.0002892132431470274,
      "loss": 7.5336,
      "step": 510
    },
    {
      "epoch": 0.18506984607171456,
      "grad_norm": 7.629673957824707,
      "learning_rate": 0.000288999644001424,
      "loss": 7.4516,
      "step": 520
    },
    {
      "epoch": 0.18862888157309368,
      "grad_norm": 8.361220359802246,
      "learning_rate": 0.0002887860448558205,
      "loss": 7.5029,
      "step": 530
    },
    {
      "epoch": 0.19218791707447283,
      "grad_norm": 6.931319713592529,
      "learning_rate": 0.0002885724457102172,
      "loss": 7.6864,
      "step": 540
    },
    {
      "epoch": 0.19574695257585195,
      "grad_norm": 7.534122943878174,
      "learning_rate": 0.0002883588465646137,
      "loss": 7.3364,
      "step": 550
    },
    {
      "epoch": 0.19930598807723107,
      "grad_norm": 7.870364189147949,
      "learning_rate": 0.0002881452474190103,
      "loss": 7.3422,
      "step": 560
    },
    {
      "epoch": 0.2028650235786102,
      "grad_norm": 8.022391319274902,
      "learning_rate": 0.00028793164827340686,
      "loss": 7.3693,
      "step": 570
    },
    {
      "epoch": 0.2064240590799893,
      "grad_norm": 7.894487380981445,
      "learning_rate": 0.00028771804912780345,
      "loss": 7.3779,
      "step": 580
    },
    {
      "epoch": 0.20998309458136846,
      "grad_norm": 8.524264335632324,
      "learning_rate": 0.00028750444998220005,
      "loss": 7.3632,
      "step": 590
    },
    {
      "epoch": 0.21354213008274758,
      "grad_norm": 8.865279197692871,
      "learning_rate": 0.0002872908508365966,
      "loss": 7.4807,
      "step": 600
    },
    {
      "epoch": 0.2171011655841267,
      "grad_norm": 6.7194437980651855,
      "learning_rate": 0.00028707725169099325,
      "loss": 7.3393,
      "step": 610
    },
    {
      "epoch": 0.22066020108550582,
      "grad_norm": 10.862590789794922,
      "learning_rate": 0.0002868636525453898,
      "loss": 7.5221,
      "step": 620
    },
    {
      "epoch": 0.22421923658688495,
      "grad_norm": 6.964584827423096,
      "learning_rate": 0.0002866500533997864,
      "loss": 7.3438,
      "step": 630
    },
    {
      "epoch": 0.22777827208826407,
      "grad_norm": 7.547250270843506,
      "learning_rate": 0.00028643645425418293,
      "loss": 7.1616,
      "step": 640
    },
    {
      "epoch": 0.23133730758964322,
      "grad_norm": 7.583577632904053,
      "learning_rate": 0.0002862228551085796,
      "loss": 7.2377,
      "step": 650
    },
    {
      "epoch": 0.23489634309102234,
      "grad_norm": 6.35359001159668,
      "learning_rate": 0.0002860092559629761,
      "loss": 7.3499,
      "step": 660
    },
    {
      "epoch": 0.23845537859240146,
      "grad_norm": 7.109849452972412,
      "learning_rate": 0.0002857956568173727,
      "loss": 7.3628,
      "step": 670
    },
    {
      "epoch": 0.24201441409378058,
      "grad_norm": 7.2586259841918945,
      "learning_rate": 0.0002855820576717693,
      "loss": 7.5106,
      "step": 680
    },
    {
      "epoch": 0.2455734495951597,
      "grad_norm": 6.855806350708008,
      "learning_rate": 0.00028536845852616586,
      "loss": 7.2835,
      "step": 690
    },
    {
      "epoch": 0.24913248509653885,
      "grad_norm": 6.830247402191162,
      "learning_rate": 0.00028515485938056245,
      "loss": 7.1989,
      "step": 700
    },
    {
      "epoch": 0.25269152059791794,
      "grad_norm": 8.174917221069336,
      "learning_rate": 0.000284941260234959,
      "loss": 7.4965,
      "step": 710
    },
    {
      "epoch": 0.2562505560992971,
      "grad_norm": 6.278923034667969,
      "learning_rate": 0.00028472766108935565,
      "loss": 7.5704,
      "step": 720
    },
    {
      "epoch": 0.25980959160067624,
      "grad_norm": 6.518519878387451,
      "learning_rate": 0.0002845140619437522,
      "loss": 7.3979,
      "step": 730
    },
    {
      "epoch": 0.26336862710205533,
      "grad_norm": 8.649812698364258,
      "learning_rate": 0.0002843004627981488,
      "loss": 7.3541,
      "step": 740
    },
    {
      "epoch": 0.2669276626034345,
      "grad_norm": 8.057802200317383,
      "learning_rate": 0.00028408686365254533,
      "loss": 7.4471,
      "step": 750
    },
    {
      "epoch": 0.27048669810481357,
      "grad_norm": 6.01824951171875,
      "learning_rate": 0.000283873264506942,
      "loss": 7.1665,
      "step": 760
    },
    {
      "epoch": 0.2740457336061927,
      "grad_norm": 6.405084133148193,
      "learning_rate": 0.0002836596653613385,
      "loss": 7.1612,
      "step": 770
    },
    {
      "epoch": 0.27760476910757187,
      "grad_norm": 6.415867805480957,
      "learning_rate": 0.0002834460662157351,
      "loss": 7.3692,
      "step": 780
    },
    {
      "epoch": 0.28116380460895096,
      "grad_norm": 6.925130844116211,
      "learning_rate": 0.0002832324670701317,
      "loss": 7.3803,
      "step": 790
    },
    {
      "epoch": 0.2847228401103301,
      "grad_norm": 7.779542922973633,
      "learning_rate": 0.00028301886792452826,
      "loss": 7.353,
      "step": 800
    },
    {
      "epoch": 0.2882818756117092,
      "grad_norm": 7.273568630218506,
      "learning_rate": 0.00028280526877892486,
      "loss": 7.2131,
      "step": 810
    },
    {
      "epoch": 0.29184091111308835,
      "grad_norm": 6.208076000213623,
      "learning_rate": 0.0002825916696333214,
      "loss": 7.2589,
      "step": 820
    },
    {
      "epoch": 0.2953999466144675,
      "grad_norm": 7.4731245040893555,
      "learning_rate": 0.00028237807048771805,
      "loss": 7.1122,
      "step": 830
    },
    {
      "epoch": 0.2989589821158466,
      "grad_norm": 8.252047538757324,
      "learning_rate": 0.0002821644713421146,
      "loss": 7.2515,
      "step": 840
    },
    {
      "epoch": 0.30251801761722574,
      "grad_norm": 8.391267776489258,
      "learning_rate": 0.0002819508721965112,
      "loss": 7.3383,
      "step": 850
    },
    {
      "epoch": 0.30607705311860484,
      "grad_norm": 7.359401226043701,
      "learning_rate": 0.0002817372730509078,
      "loss": 7.5605,
      "step": 860
    },
    {
      "epoch": 0.309636088619984,
      "grad_norm": 6.828577995300293,
      "learning_rate": 0.0002815236739053044,
      "loss": 7.5237,
      "step": 870
    },
    {
      "epoch": 0.31319512412136313,
      "grad_norm": 7.90355920791626,
      "learning_rate": 0.00028131007475970093,
      "loss": 7.2835,
      "step": 880
    },
    {
      "epoch": 0.3167541596227422,
      "grad_norm": 5.705327987670898,
      "learning_rate": 0.0002810964756140975,
      "loss": 7.2743,
      "step": 890
    },
    {
      "epoch": 0.3203131951241214,
      "grad_norm": 6.340001106262207,
      "learning_rate": 0.0002808828764684941,
      "loss": 7.4354,
      "step": 900
    },
    {
      "epoch": 0.32387223062550047,
      "grad_norm": 6.021364688873291,
      "learning_rate": 0.00028066927732289066,
      "loss": 7.2606,
      "step": 910
    },
    {
      "epoch": 0.3274312661268796,
      "grad_norm": 6.157351970672607,
      "learning_rate": 0.00028045567817728726,
      "loss": 7.2453,
      "step": 920
    },
    {
      "epoch": 0.33099030162825877,
      "grad_norm": 7.711866855621338,
      "learning_rate": 0.00028024207903168386,
      "loss": 7.3397,
      "step": 930
    },
    {
      "epoch": 0.33454933712963786,
      "grad_norm": 6.916357040405273,
      "learning_rate": 0.00028002847988608045,
      "loss": 7.4285,
      "step": 940
    },
    {
      "epoch": 0.338108372631017,
      "grad_norm": 6.4691033363342285,
      "learning_rate": 0.000279814880740477,
      "loss": 7.274,
      "step": 950
    },
    {
      "epoch": 0.3416674081323961,
      "grad_norm": 5.9838032722473145,
      "learning_rate": 0.0002796012815948736,
      "loss": 7.0844,
      "step": 960
    },
    {
      "epoch": 0.34522644363377525,
      "grad_norm": 6.300545692443848,
      "learning_rate": 0.0002793876824492702,
      "loss": 7.3134,
      "step": 970
    },
    {
      "epoch": 0.3487854791351544,
      "grad_norm": 6.785462379455566,
      "learning_rate": 0.0002791740833036668,
      "loss": 6.9639,
      "step": 980
    },
    {
      "epoch": 0.3523445146365335,
      "grad_norm": 6.412263870239258,
      "learning_rate": 0.00027896048415806333,
      "loss": 7.1907,
      "step": 990
    },
    {
      "epoch": 0.35590355013791264,
      "grad_norm": 6.74686336517334,
      "learning_rate": 0.00027874688501245993,
      "loss": 7.3041,
      "step": 1000
    },
    {
      "epoch": 0.35946258563929173,
      "grad_norm": 6.666254997253418,
      "learning_rate": 0.0002785332858668565,
      "loss": 7.2788,
      "step": 1010
    },
    {
      "epoch": 0.3630216211406709,
      "grad_norm": 7.963587760925293,
      "learning_rate": 0.00027831968672125307,
      "loss": 6.9715,
      "step": 1020
    },
    {
      "epoch": 0.36658065664205003,
      "grad_norm": 5.82476282119751,
      "learning_rate": 0.00027810608757564966,
      "loss": 7.2089,
      "step": 1030
    },
    {
      "epoch": 0.3701396921434291,
      "grad_norm": 5.950572967529297,
      "learning_rate": 0.00027789248843004626,
      "loss": 7.2169,
      "step": 1040
    },
    {
      "epoch": 0.37369872764480827,
      "grad_norm": 6.356487274169922,
      "learning_rate": 0.00027767888928444286,
      "loss": 6.9708,
      "step": 1050
    },
    {
      "epoch": 0.37725776314618736,
      "grad_norm": 6.221110820770264,
      "learning_rate": 0.0002774652901388394,
      "loss": 7.218,
      "step": 1060
    },
    {
      "epoch": 0.3808167986475665,
      "grad_norm": 6.460140228271484,
      "learning_rate": 0.000277251690993236,
      "loss": 7.01,
      "step": 1070
    },
    {
      "epoch": 0.38437583414894566,
      "grad_norm": 8.184578895568848,
      "learning_rate": 0.0002770380918476326,
      "loss": 6.9083,
      "step": 1080
    },
    {
      "epoch": 0.38793486965032475,
      "grad_norm": 5.865383148193359,
      "learning_rate": 0.0002768244927020292,
      "loss": 7.0647,
      "step": 1090
    },
    {
      "epoch": 0.3914939051517039,
      "grad_norm": 6.50124454498291,
      "learning_rate": 0.00027661089355642573,
      "loss": 7.2032,
      "step": 1100
    },
    {
      "epoch": 0.395052940653083,
      "grad_norm": 7.081464767456055,
      "learning_rate": 0.00027639729441082233,
      "loss": 6.9506,
      "step": 1110
    },
    {
      "epoch": 0.39861197615446214,
      "grad_norm": 6.305020332336426,
      "learning_rate": 0.00027618369526521893,
      "loss": 7.3341,
      "step": 1120
    },
    {
      "epoch": 0.4021710116558413,
      "grad_norm": 5.619619846343994,
      "learning_rate": 0.00027597009611961547,
      "loss": 7.1928,
      "step": 1130
    },
    {
      "epoch": 0.4057300471572204,
      "grad_norm": 6.890651702880859,
      "learning_rate": 0.00027575649697401207,
      "loss": 6.9639,
      "step": 1140
    },
    {
      "epoch": 0.40928908265859953,
      "grad_norm": 6.946043014526367,
      "learning_rate": 0.00027554289782840866,
      "loss": 7.0193,
      "step": 1150
    },
    {
      "epoch": 0.4128481181599786,
      "grad_norm": 7.5354084968566895,
      "learning_rate": 0.00027532929868280526,
      "loss": 7.3595,
      "step": 1160
    },
    {
      "epoch": 0.4164071536613578,
      "grad_norm": 6.631800651550293,
      "learning_rate": 0.0002751156995372018,
      "loss": 7.0902,
      "step": 1170
    },
    {
      "epoch": 0.4199661891627369,
      "grad_norm": 7.803922176361084,
      "learning_rate": 0.0002749021003915984,
      "loss": 7.0113,
      "step": 1180
    },
    {
      "epoch": 0.423525224664116,
      "grad_norm": 7.533486843109131,
      "learning_rate": 0.000274688501245995,
      "loss": 7.2607,
      "step": 1190
    },
    {
      "epoch": 0.42708426016549517,
      "grad_norm": 6.246283531188965,
      "learning_rate": 0.0002744749021003916,
      "loss": 6.9414,
      "step": 1200
    },
    {
      "epoch": 0.43064329566687426,
      "grad_norm": 6.813659191131592,
      "learning_rate": 0.00027426130295478814,
      "loss": 7.0926,
      "step": 1210
    },
    {
      "epoch": 0.4342023311682534,
      "grad_norm": 5.953593730926514,
      "learning_rate": 0.00027404770380918473,
      "loss": 7.1528,
      "step": 1220
    },
    {
      "epoch": 0.43776136666963256,
      "grad_norm": 5.274199485778809,
      "learning_rate": 0.00027383410466358133,
      "loss": 7.2268,
      "step": 1230
    },
    {
      "epoch": 0.44132040217101165,
      "grad_norm": 6.160034656524658,
      "learning_rate": 0.0002736205055179779,
      "loss": 7.1112,
      "step": 1240
    },
    {
      "epoch": 0.4448794376723908,
      "grad_norm": 6.093071460723877,
      "learning_rate": 0.0002734069063723745,
      "loss": 7.0027,
      "step": 1250
    },
    {
      "epoch": 0.4484384731737699,
      "grad_norm": 5.594979286193848,
      "learning_rate": 0.00027319330722677107,
      "loss": 6.8885,
      "step": 1260
    },
    {
      "epoch": 0.45199750867514904,
      "grad_norm": 7.348594665527344,
      "learning_rate": 0.00027297970808116766,
      "loss": 7.1712,
      "step": 1270
    },
    {
      "epoch": 0.45555654417652813,
      "grad_norm": 6.347436428070068,
      "learning_rate": 0.0002727661089355642,
      "loss": 6.8982,
      "step": 1280
    },
    {
      "epoch": 0.4591155796779073,
      "grad_norm": 6.925621032714844,
      "learning_rate": 0.0002725525097899608,
      "loss": 7.214,
      "step": 1290
    },
    {
      "epoch": 0.46267461517928643,
      "grad_norm": 6.245063781738281,
      "learning_rate": 0.0002723389106443574,
      "loss": 7.1777,
      "step": 1300
    },
    {
      "epoch": 0.4662336506806655,
      "grad_norm": 6.893316745758057,
      "learning_rate": 0.000272125311498754,
      "loss": 7.0093,
      "step": 1310
    },
    {
      "epoch": 0.46979268618204467,
      "grad_norm": 5.7352118492126465,
      "learning_rate": 0.0002719117123531506,
      "loss": 7.0842,
      "step": 1320
    },
    {
      "epoch": 0.47335172168342377,
      "grad_norm": 6.046491622924805,
      "learning_rate": 0.00027169811320754714,
      "loss": 7.0082,
      "step": 1330
    },
    {
      "epoch": 0.4769107571848029,
      "grad_norm": 5.684953212738037,
      "learning_rate": 0.00027148451406194373,
      "loss": 7.1322,
      "step": 1340
    },
    {
      "epoch": 0.48046979268618206,
      "grad_norm": 6.5879926681518555,
      "learning_rate": 0.0002712709149163403,
      "loss": 6.9477,
      "step": 1350
    },
    {
      "epoch": 0.48402882818756116,
      "grad_norm": 5.26768684387207,
      "learning_rate": 0.00027105731577073693,
      "loss": 7.0061,
      "step": 1360
    },
    {
      "epoch": 0.4875878636889403,
      "grad_norm": 6.101306438446045,
      "learning_rate": 0.00027084371662513347,
      "loss": 7.2341,
      "step": 1370
    },
    {
      "epoch": 0.4911468991903194,
      "grad_norm": 5.804507255554199,
      "learning_rate": 0.00027063011747953007,
      "loss": 7.0396,
      "step": 1380
    },
    {
      "epoch": 0.49470593469169855,
      "grad_norm": 7.029499530792236,
      "learning_rate": 0.00027041651833392666,
      "loss": 6.9261,
      "step": 1390
    },
    {
      "epoch": 0.4982649701930777,
      "grad_norm": 6.767526626586914,
      "learning_rate": 0.0002702029191883232,
      "loss": 6.8992,
      "step": 1400
    },
    {
      "epoch": 0.5018240056944568,
      "grad_norm": 6.183251857757568,
      "learning_rate": 0.0002699893200427198,
      "loss": 7.0483,
      "step": 1410
    },
    {
      "epoch": 0.5053830411958359,
      "grad_norm": 6.538424968719482,
      "learning_rate": 0.0002697757208971164,
      "loss": 7.0807,
      "step": 1420
    },
    {
      "epoch": 0.508942076697215,
      "grad_norm": 5.450777053833008,
      "learning_rate": 0.000269562121751513,
      "loss": 6.9174,
      "step": 1430
    },
    {
      "epoch": 0.5125011121985942,
      "grad_norm": 7.336591720581055,
      "learning_rate": 0.00026934852260590954,
      "loss": 6.918,
      "step": 1440
    },
    {
      "epoch": 0.5160601476999733,
      "grad_norm": 5.683486461639404,
      "learning_rate": 0.00026913492346030614,
      "loss": 7.1814,
      "step": 1450
    },
    {
      "epoch": 0.5196191832013525,
      "grad_norm": 8.969990730285645,
      "learning_rate": 0.0002689213243147027,
      "loss": 7.1203,
      "step": 1460
    },
    {
      "epoch": 0.5231782187027315,
      "grad_norm": 5.5265212059021,
      "learning_rate": 0.00026870772516909933,
      "loss": 6.9093,
      "step": 1470
    },
    {
      "epoch": 0.5267372542041107,
      "grad_norm": 6.137478828430176,
      "learning_rate": 0.0002684941260234959,
      "loss": 6.9023,
      "step": 1480
    },
    {
      "epoch": 0.5302962897054898,
      "grad_norm": 7.175308704376221,
      "learning_rate": 0.00026828052687789247,
      "loss": 6.9106,
      "step": 1490
    },
    {
      "epoch": 0.533855325206869,
      "grad_norm": 5.734164237976074,
      "learning_rate": 0.00026806692773228907,
      "loss": 7.0353,
      "step": 1500
    },
    {
      "epoch": 0.5374143607082481,
      "grad_norm": 7.776902675628662,
      "learning_rate": 0.0002678533285866856,
      "loss": 7.0596,
      "step": 1510
    },
    {
      "epoch": 0.5409733962096271,
      "grad_norm": 5.591251850128174,
      "learning_rate": 0.0002676397294410822,
      "loss": 6.9908,
      "step": 1520
    },
    {
      "epoch": 0.5445324317110063,
      "grad_norm": 7.013352394104004,
      "learning_rate": 0.0002674261302954788,
      "loss": 6.9821,
      "step": 1530
    },
    {
      "epoch": 0.5480914672123854,
      "grad_norm": 7.402930736541748,
      "learning_rate": 0.0002672125311498754,
      "loss": 7.0288,
      "step": 1540
    },
    {
      "epoch": 0.5516505027137646,
      "grad_norm": 4.754340648651123,
      "learning_rate": 0.00026699893200427194,
      "loss": 6.9489,
      "step": 1550
    },
    {
      "epoch": 0.5552095382151437,
      "grad_norm": 7.284138202667236,
      "learning_rate": 0.00026678533285866854,
      "loss": 7.085,
      "step": 1560
    },
    {
      "epoch": 0.5587685737165228,
      "grad_norm": 5.845667839050293,
      "learning_rate": 0.00026657173371306514,
      "loss": 6.9411,
      "step": 1570
    },
    {
      "epoch": 0.5623276092179019,
      "grad_norm": 4.732044696807861,
      "learning_rate": 0.00026635813456746173,
      "loss": 6.7878,
      "step": 1580
    },
    {
      "epoch": 0.5658866447192811,
      "grad_norm": 6.444329261779785,
      "learning_rate": 0.0002661445354218583,
      "loss": 6.8888,
      "step": 1590
    },
    {
      "epoch": 0.5694456802206602,
      "grad_norm": 6.919723033905029,
      "learning_rate": 0.0002659309362762549,
      "loss": 6.9604,
      "step": 1600
    },
    {
      "epoch": 0.5730047157220394,
      "grad_norm": 8.272061347961426,
      "learning_rate": 0.00026571733713065147,
      "loss": 6.9111,
      "step": 1610
    },
    {
      "epoch": 0.5765637512234184,
      "grad_norm": 6.577447414398193,
      "learning_rate": 0.000265503737985048,
      "loss": 6.8877,
      "step": 1620
    },
    {
      "epoch": 0.5801227867247976,
      "grad_norm": 6.5638427734375,
      "learning_rate": 0.0002652901388394446,
      "loss": 7.32,
      "step": 1630
    },
    {
      "epoch": 0.5836818222261767,
      "grad_norm": 6.173512935638428,
      "learning_rate": 0.0002650765396938412,
      "loss": 6.9067,
      "step": 1640
    },
    {
      "epoch": 0.5872408577275559,
      "grad_norm": 6.210359573364258,
      "learning_rate": 0.0002648629405482378,
      "loss": 6.8803,
      "step": 1650
    },
    {
      "epoch": 0.590799893228935,
      "grad_norm": 6.480352878570557,
      "learning_rate": 0.00026464934140263435,
      "loss": 6.9787,
      "step": 1660
    },
    {
      "epoch": 0.594358928730314,
      "grad_norm": 5.187366962432861,
      "learning_rate": 0.00026443574225703094,
      "loss": 6.9779,
      "step": 1670
    },
    {
      "epoch": 0.5979179642316932,
      "grad_norm": 6.26608419418335,
      "learning_rate": 0.00026422214311142754,
      "loss": 6.7221,
      "step": 1680
    },
    {
      "epoch": 0.6014769997330723,
      "grad_norm": 6.453166484832764,
      "learning_rate": 0.00026400854396582414,
      "loss": 7.0504,
      "step": 1690
    },
    {
      "epoch": 0.6050360352344515,
      "grad_norm": 5.328619003295898,
      "learning_rate": 0.0002637949448202207,
      "loss": 6.9793,
      "step": 1700
    },
    {
      "epoch": 0.6085950707358306,
      "grad_norm": 5.783942699432373,
      "learning_rate": 0.0002635813456746173,
      "loss": 7.1049,
      "step": 1710
    },
    {
      "epoch": 0.6121541062372097,
      "grad_norm": 4.297878742218018,
      "learning_rate": 0.0002633677465290139,
      "loss": 6.7982,
      "step": 1720
    },
    {
      "epoch": 0.6157131417385888,
      "grad_norm": 5.950531005859375,
      "learning_rate": 0.0002631541473834104,
      "loss": 7.0737,
      "step": 1730
    },
    {
      "epoch": 0.619272177239968,
      "grad_norm": 5.23029088973999,
      "learning_rate": 0.000262940548237807,
      "loss": 7.0912,
      "step": 1740
    },
    {
      "epoch": 0.6228312127413471,
      "grad_norm": 6.766277313232422,
      "learning_rate": 0.0002627269490922036,
      "loss": 6.8457,
      "step": 1750
    },
    {
      "epoch": 0.6263902482427263,
      "grad_norm": 7.065883636474609,
      "learning_rate": 0.0002625133499466002,
      "loss": 7.0386,
      "step": 1760
    },
    {
      "epoch": 0.6299492837441053,
      "grad_norm": 5.633376598358154,
      "learning_rate": 0.00026229975080099675,
      "loss": 6.8616,
      "step": 1770
    },
    {
      "epoch": 0.6335083192454845,
      "grad_norm": 4.6550140380859375,
      "learning_rate": 0.00026208615165539335,
      "loss": 6.7808,
      "step": 1780
    },
    {
      "epoch": 0.6370673547468636,
      "grad_norm": 5.580455303192139,
      "learning_rate": 0.00026187255250978994,
      "loss": 6.7421,
      "step": 1790
    },
    {
      "epoch": 0.6406263902482427,
      "grad_norm": 5.204733371734619,
      "learning_rate": 0.00026165895336418654,
      "loss": 7.001,
      "step": 1800
    },
    {
      "epoch": 0.6441854257496219,
      "grad_norm": 5.0276408195495605,
      "learning_rate": 0.0002614453542185831,
      "loss": 6.9071,
      "step": 1810
    },
    {
      "epoch": 0.6477444612510009,
      "grad_norm": 6.883731365203857,
      "learning_rate": 0.0002612317550729797,
      "loss": 6.6523,
      "step": 1820
    },
    {
      "epoch": 0.6513034967523801,
      "grad_norm": 5.961007118225098,
      "learning_rate": 0.0002610181559273763,
      "loss": 6.6849,
      "step": 1830
    },
    {
      "epoch": 0.6548625322537592,
      "grad_norm": 8.809602737426758,
      "learning_rate": 0.0002608045567817728,
      "loss": 6.9179,
      "step": 1840
    },
    {
      "epoch": 0.6584215677551384,
      "grad_norm": 5.512296676635742,
      "learning_rate": 0.0002605909576361694,
      "loss": 6.7897,
      "step": 1850
    },
    {
      "epoch": 0.6619806032565175,
      "grad_norm": 4.609118938446045,
      "learning_rate": 0.000260377358490566,
      "loss": 6.9372,
      "step": 1860
    },
    {
      "epoch": 0.6655396387578966,
      "grad_norm": 5.619687080383301,
      "learning_rate": 0.0002601637593449626,
      "loss": 6.8715,
      "step": 1870
    },
    {
      "epoch": 0.6690986742592757,
      "grad_norm": 6.225467205047607,
      "learning_rate": 0.00025995016019935915,
      "loss": 6.9298,
      "step": 1880
    },
    {
      "epoch": 0.6726577097606549,
      "grad_norm": 5.998307704925537,
      "learning_rate": 0.00025973656105375575,
      "loss": 6.8934,
      "step": 1890
    },
    {
      "epoch": 0.676216745262034,
      "grad_norm": 7.3521342277526855,
      "learning_rate": 0.00025952296190815235,
      "loss": 6.8088,
      "step": 1900
    },
    {
      "epoch": 0.6797757807634132,
      "grad_norm": 5.879306316375732,
      "learning_rate": 0.00025930936276254894,
      "loss": 6.7014,
      "step": 1910
    },
    {
      "epoch": 0.6833348162647922,
      "grad_norm": 5.97050142288208,
      "learning_rate": 0.0002590957636169455,
      "loss": 6.9852,
      "step": 1920
    },
    {
      "epoch": 0.6868938517661713,
      "grad_norm": 5.9882988929748535,
      "learning_rate": 0.0002588821644713421,
      "loss": 6.8064,
      "step": 1930
    },
    {
      "epoch": 0.6904528872675505,
      "grad_norm": 5.918356418609619,
      "learning_rate": 0.0002586685653257387,
      "loss": 6.9208,
      "step": 1940
    },
    {
      "epoch": 0.6940119227689296,
      "grad_norm": 7.0181565284729,
      "learning_rate": 0.0002584549661801352,
      "loss": 6.8427,
      "step": 1950
    },
    {
      "epoch": 0.6975709582703088,
      "grad_norm": 6.932168960571289,
      "learning_rate": 0.0002582413670345319,
      "loss": 6.9015,
      "step": 1960
    },
    {
      "epoch": 0.7011299937716878,
      "grad_norm": 6.377872467041016,
      "learning_rate": 0.0002580277678889284,
      "loss": 6.8251,
      "step": 1970
    },
    {
      "epoch": 0.704689029273067,
      "grad_norm": 4.70603609085083,
      "learning_rate": 0.000257814168743325,
      "loss": 6.9055,
      "step": 1980
    },
    {
      "epoch": 0.7082480647744461,
      "grad_norm": 5.841550350189209,
      "learning_rate": 0.00025760056959772156,
      "loss": 6.9828,
      "step": 1990
    },
    {
      "epoch": 0.7118071002758253,
      "grad_norm": 5.855922698974609,
      "learning_rate": 0.0002573869704521182,
      "loss": 6.997,
      "step": 2000
    },
    {
      "epoch": 0.7118071002758253,
      "eval_loss": 0.8709157109260559,
      "eval_runtime": 847.115,
      "eval_samples_per_second": 0.118,
      "eval_steps_per_second": 0.015,
      "step": 2000
    }
  ],
  "logging_steps": 10,
  "max_steps": 14045,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 2000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4139761803395072e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
